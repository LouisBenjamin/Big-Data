{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data: https://www.kaggle.com/pdunton/marvel-cinematic-universe-dialogue?select=mcu_subset.csv\n",
    "data NRC : https://www.kaggle.com/andradaolteanu/bing-nrc-afinn-lexicons?select=NRC.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover, RegexTokenizer, PCA\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from IPython.display import Image\n",
    "from pyspark.sql import SparkSession\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting The Infinity Stones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AKA Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![display image](https://media.giphy.com/media/3oxHQjRHcp4w9oi24M/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()\n",
    "#read_csv = spark.read.csv('data/tweets.csv', inferSchema=True, header=True)\n",
    "read_csv = spark.read.csv('data/Reddit_Data_utf8.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = read_csv.select(\"SentimentText\", col(\"Sentiment\").cast(\"Int\").alias(\"label\"))\n",
    "data = read_csv.select(\"clean_comment\", col(\"category\").cast(\"Int\").alias(\"label\")).dropna().dropDuplicates().replace(-1,2).limit(15000)\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = data.randomSplit([0.7, 0.3])\n",
    "trainingData = split[0]\n",
    "testingData = split[1]\n",
    "print (\"Training data has\", split[0].count(), 'rows.')\n",
    "print (\"Testing data has\", split[1].count(), 'rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data (Tokenizing and Stop Word Removing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputCol = \"SentimentText\"\n",
    "inputCol = \"clean_comment\"\n",
    "\n",
    "tokenizer = RegexTokenizer(pattern=r'(?:\\p{Punct}|\\s)+', inputCol=inputCol, outputCol=\"Tokens\")\n",
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"NoStopWords\")\n",
    "\n",
    "token_train = tokenizer.transform(trainingData)\n",
    "nosw_train = swr.transform(token_train)\n",
    "\n",
    "token_test = tokenizer.transform(testingData)\n",
    "nosw_test = swr.transform(token_test)\n",
    "\n",
    "nosw_train.show(truncate=True, n=10)\n",
    "nosw_test.show(truncate=True, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing The Features using HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "hash_train = hashTF.transform(nosw_train).select(\n",
    "    'label', 'Tokens', 'features')\n",
    "\n",
    "hash_test = hashTF.transform(nosw_test).select(\n",
    "    'Label', 'Tokens', 'features')\n",
    "hash_train.show(n=5)\n",
    "hash_test.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlor = (LogisticRegression()\n",
    "       .setFamily(\"multinomial\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= mlor.fit(hash_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = model.transform(hash_test)\n",
    "prediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionFinal = prediction.select(\n",
    "    \"Tokens\", \"prediction\", \"Label\")\n",
    "predictionFinal.show(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = predictionFinal.filter(predictionFinal['prediction'] == predictionFinal['label']).count()\n",
    "total = predictionFinal.count()\n",
    "print(\"Accuracy:\", match/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avengers Assemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![display image](https://media.giphy.com/media/j2pWZpr5RlpCodOB0d/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcu_csv = spark.read.csv('data/mcu_subset.csv', inferSchema=True, header=True)\n",
    "print(\"Lines of Dialogue:\", mcu_csv.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mcu_csv.select(\"character\",\"line\")\n",
    "data.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(inputCol=\"line\", outputCol=\"new_line\")\n",
    "swr_MCU = StopWordsRemover(inputCol=t.getOutputCol(), \n",
    "                       outputCol=\"new\")\n",
    "token_MCU = t.transform(data)\n",
    "nosw_MCU = swr_MCU.transform(token_MCU)\n",
    "\n",
    "nosw_MCU.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashTF = HashingTF(inputCol=swr_MCU.getOutputCol(), outputCol=\"features\")\n",
    "hash_MCU = hashTF.transform(SwRemovedMCU).select('new_line', 'features')\n",
    "hash_MCU.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(numeric_MCU)\n",
    "predictionFinal_mcu = prediction.select(\n",
    "    \"new_line\", \"prediction\")\n",
    "predictionFinal_mcu.show(n=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = predictionFinal_mcu.groupBy('prediction').count()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
