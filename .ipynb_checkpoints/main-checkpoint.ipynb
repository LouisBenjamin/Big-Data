{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data: https://www.kaggle.com/pdunton/marvel-cinematic-universe-dialogue?select=mcu_subset.csv\n",
    "data NRC : https://www.kaggle.com/andradaolteanu/bing-nrc-afinn-lexicons?select=NRC.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover, RegexTokenizer\n",
    "from IPython.display import Image\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting The Infinity Stones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AKA Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![display image](https://media.giphy.com/media/3oxHQjRHcp4w9oi24M/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()\n",
    "#read_csv = spark.read.csv('data/tweets.csv', inferSchema=True, header=True)\n",
    "read_csv = spark.read.csv('data/Reddit_Data_utf8.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|       clean_comment|label|\n",
      "+--------------------+-----+\n",
      "|surprised modis i...|    1|\n",
      "|     naga downs ing |    0|\n",
      "| has been decided...|    0|\n",
      "|how difficult was...|    1|\n",
      "|yes now are going...|    0|\n",
      "|every question yo...|    0|\n",
      "|  the plot thickens |    0|\n",
      "|this compelling y...|   -1|\n",
      "|video showing mh3...|    0|\n",
      "|brace yourself th...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data = read_csv.select(\"SentimentText\", col(\"Sentiment\").cast(\"Int\").alias(\"label\"))\n",
    "data = read_csv.select(\"clean_comment\", col(\"category\").cast(\"Int\").alias(\"label\")).dropna().dropDuplicates()\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 25465 rows.\n",
      "Testing data has 11074 rows.\n"
     ]
    }
   ],
   "source": [
    "split = data.randomSplit([0.7, 0.3])\n",
    "trainingData = split[0]\n",
    "testingData = split[1]\n",
    "print (\"Training data has\", split[0].count(), 'rows.')\n",
    "print (\"Testing data has\", split[1].count(), 'rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data (Tokenizing and Stop Word Removing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|       clean_comment|label|              Tokens|         NoStopWords|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "| 5ppr for flex sp...|    0|[5ppr, for, flex,...|[5ppr, flex, spot...|\n",
      "| all the signs em...|    1|[all, the, signs,...|[signs, emergency...|\n",
      "| assholery has fa...|    0|[assholery, has, ...|[assholery, face,...|\n",
      "| back vent here a...|    0|[back, vent, here...|  [back, vent, lose]|\n",
      "| big political le...|    1|[big, political, ...|[big, political, ...|\n",
      "| bjp will proacti...|   -1|[bjp, will, proac...|[bjp, proactive, ...|\n",
      "| bunch aap suppor...|   -1|[bunch, aap, supp...|[bunch, aap, supp...|\n",
      "| couldn see havin...|    1|[couldn, see, hav...|[couldn, see, pos...|\n",
      "| doing decent job...|    1|[doing, decent, j...|[decent, job, com...|\n",
      "| don know ethical...|    1|[don, know, ethic...|[know, ethical, f...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|       clean_comment|label|              Tokens|         NoStopWords|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "| are you saying t...|    0|[are, you, saying...|[saying, tally, b...|\n",
      "| because fucking ...|    0|[because, fucking...|[fucking, white, ...|\n",
      "| breaking news ma...|    0|[breaking, news, ...|[breaking, news, ...|\n",
      "| feel bad for the...|   -1|[feel, bad, for, ...|[feel, bad, woode...|\n",
      "| hai aam modi aap...|   -1|[hai, aam, modi, ...|[hai, aam, modi, ...|\n",
      "| has been decided...|    0|[has, been, decid...|      [decided, jpg]|\n",
      "| hope gets paytm ...|    1|[hope, gets, payt...|[hope, gets, payt...|\n",
      "| india makes soft...|    0|[india, makes, so...|[india, makes, so...|\n",
      "| need bjp mukt bh...|    0|[need, bjp, mukt,...|[need, bjp, mukt,...|\n",
      "| press conference...|    0|[press, conferenc...|[press, conferenc...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inputCol = \"SentimentText\"\n",
    "inputCol = \"clean_comment\"\n",
    "\n",
    "tokenizer = RegexTokenizer(pattern=r'(?:\\p{Punct}|\\s)+', inputCol=inputCol, outputCol=\"Tokens\")\n",
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"NoStopWords\")\n",
    "\n",
    "token_train = tokenizer.transform(trainingData)\n",
    "nosw_train = swr.transform(token_train)\n",
    "\n",
    "token_test = tokenizer.transform(testingData)\n",
    "nosw_test = swr.transform(token_test)\n",
    "\n",
    "nosw_train.show(truncate=True, n=10)\n",
    "nosw_test.show(truncate=True, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing The Features using HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|              Tokens|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[5ppr, for, flex,...|(262144,[5972,147...|\n",
      "|    1|[all, the, signs,...|(262144,[2306,256...|\n",
      "|    0|[assholery, has, ...|(262144,[13644,20...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+--------------------+--------------------+\n",
      "|Label|              Tokens|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[are, you, saying...|(262144,[160395,1...|\n",
      "|    0|[because, fucking...|(262144,[41748,43...|\n",
      "|    0|[breaking, news, ...|(262144,[18981,27...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "hash_train = hashTF.transform(nosw_train).select(\n",
    "    'label', 'Tokens', 'features')\n",
    "\n",
    "hash_test = hashTF.transform(nosw_test).select(\n",
    "    'Label', 'Tokens', 'features')\n",
    "hash_train.show(n=3)\n",
    "hash_test.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-502-c34282104283>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10, regParam=0.01)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhash_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegressionWithLBFGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#model = lr.fit(hash_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\mllib\\classification.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, data, iterations, initialWeights, regParam, regType, intercept, corrections, tolerance, validateData, numClasses)\u001b[0m\n\u001b[0;32m    397\u001b[0m                     \u001b[0minitialWeights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnumClasses\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m                     \u001b[0minitialWeights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnumClasses\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'features'"
     ]
    }
   ],
   "source": [
    "#lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10, regParam=0.01)\n",
    "rdd = hash_train.rdd.map(tuple)\n",
    "model = LogisticRegressionWithLBFGS.train(rdd, numClasses=3)\n",
    "#model = lr.fit(hash_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+----------+-----+\n",
      "|Tokens                             |prediction|Label|\n",
      "+-----------------------------------+----------+-----+\n",
      "|[i, adore, cheese, #toptastic]     |1.0       |1    |\n",
      "|[i, adore, jam, #brilliant]        |1.0       |1    |\n",
      "|[i, adore, jam, #thumbs-up]        |1.0       |1    |\n",
      "|[i, adore, jam, #toptastic]        |1.0       |1    |\n",
      "|[i, adore, skiing, #brilliant]     |1.0       |1    |\n",
      "|[i, adore, summer, #thumbs-up]     |1.0       |1    |\n",
      "|[i, adore, summer, #toptastic]     |1.0       |1    |\n",
      "|[i, adore, tea, #thumbs-up]        |1.0       |1    |\n",
      "|[i, adore, that, band, #loveit]    |1.0       |1    |\n",
      "|[i, adore, that, movie, #toptastic]|1.0       |0    |\n",
      "|[i, adore, this, book, #bestever]  |1.0       |1    |\n",
      "|[i, adore, this, game, #favorite]  |1.0       |1    |\n",
      "|[i, adore, this, game, #loveit]    |1.0       |1    |\n",
      "|[i, adore, winter, #brilliant]     |1.0       |1    |\n",
      "|[i, adore, winter, #loveit]        |1.0       |1    |\n",
      "|[i, adore, winter, #toptastic]     |1.0       |1    |\n",
      "|[i, dislike, cheese, #nightmare]   |0.0       |0    |\n",
      "|[i, dislike, jam, #rubbish]        |0.0       |0    |\n",
      "|[i, dislike, pop, music, #fail]    |0.0       |0    |\n",
      "|[i, dislike, rock, music, #hateit] |0.0       |0    |\n",
      "+-----------------------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(numericTest)\n",
    "predictionFinal = prediction.select(\n",
    "    \"Tokens\", \"prediction\", \"Label\")\n",
    "predictionFinal.show(n=20, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9867021276595744\n",
      "True Negative: 196\n",
      "True Positive: 175\n"
     ]
    }
   ],
   "source": [
    "tn = predictionFinal.filter((predictionFinal['prediction'] == 0) & (predictionFinal['label'] == 0)).count()\n",
    "tp = predictionFinal.filter((predictionFinal['prediction'] == 1) & (predictionFinal['label'] == 1)).count()\n",
    "total = predictionFinal.count()\n",
    "print(\"Accuracy:\", (tn+tp)/total)\n",
    "print(\"True Negative:\", tn)\n",
    "print(\"True Positive:\", tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avengers Assemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![display image](https://media.giphy.com/media/j2pWZpr5RlpCodOB0d/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines of Dialogue: 6509\n"
     ]
    }
   ],
   "source": [
    "mcu_csv = spark.read.csv('data/mcu_subset.csv', inferSchema=True, header=True)\n",
    "print(\"Lines of Dialogue:\", mcu_csv.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|   character|                line|\n",
      "+------------+--------------------+\n",
      "|  TONY STARK|Oh, I get it.  Yo...|\n",
      "|  TONY STARK|Oh.  I see.  So i...|\n",
      "|  TONY STARK|Good God, you’re ...|\n",
      "|  TONY STARK|             Please.|\n",
      "|  TONY STARK|Excellent questio...|\n",
      "|  TONY STARK|      Join the club.|\n",
      "|  TONY STARK|Are you aware tha...|\n",
      "|JAMES RHODES|GET DOWN, TONY.  ...|\n",
      "|JAMES RHODES|As Program Manage...|\n",
      "|  TONY STARK|...you think we’r...|\n",
      "+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = mcu_csv.select(\"character\",\"line\")\n",
    "data.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+\n",
      "|   character|                line|            new_line|                 new|\n",
      "+------------+--------------------+--------------------+--------------------+\n",
      "|  TONY STARK|Oh, I get it.  Yo...|[oh,, i, get, it....|[oh,, get, it., ,...|\n",
      "|  TONY STARK|Oh.  I see.  So i...|[oh., , i, see., ...|[oh., , see., , i...|\n",
      "|  TONY STARK|Good God, you’re ...|[good, god,, you’...|[good, god,, you’...|\n",
      "|  TONY STARK|             Please.|           [please.]|           [please.]|\n",
      "|  TONY STARK|Excellent questio...|[excellent, quest...|[excellent, quest...|\n",
      "|  TONY STARK|      Join the club.|  [join, the, club.]|       [join, club.]|\n",
      "|  TONY STARK|Are you aware tha...|[are, you, aware,...|[aware, native, a...|\n",
      "|JAMES RHODES|GET DOWN, TONY.  ...|[get, down,, tony...|[get, down,, tony...|\n",
      "|JAMES RHODES|As Program Manage...|[as, program, man...|[program, manager...|\n",
      "|  TONY STARK|...you think we’r...|[...you, think, w...|[...you, think, w...|\n",
      "+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer(inputCol=\"line\", outputCol=\"new_line\")\n",
    "swr_MCU = StopWordsRemover(inputCol=t.getOutputCol(), \n",
    "                       outputCol=\"new\")\n",
    "token_MCU = t.transform(data)\n",
    "nosw_MCU = swr_MCU.transform(token_MCU)\n",
    "\n",
    "nosw_MCU.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            new_line|            features|\n",
      "+--------------------+--------------------+\n",
      "|[oh,, i, get, it....|(262144,[44954,84...|\n",
      "|[oh., , i, see., ...|(262144,[8938,109...|\n",
      "|[good, god,, you’...|(262144,[6808,353...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashTF = HashingTF(inputCol=swr_MCU.getOutputCol(), outputCol=\"features\")\n",
    "hash_MCU = hashTF.transform(SwRemovedMCU).select('new_line', 'features')\n",
    "hash_MCU.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            new_line|prediction|\n",
      "+--------------------+----------+\n",
      "|[oh,, i, get, it....|       0.0|\n",
      "|[oh., , i, see., ...|       0.0|\n",
      "|[good, god,, you’...|       0.0|\n",
      "|           [please.]|       0.0|\n",
      "|[excellent, quest...|       0.0|\n",
      "|  [join, the, club.]|       0.0|\n",
      "|[are, you, aware,...|       0.0|\n",
      "|[get, down,, tony...|       0.0|\n",
      "|[as, program, man...|       1.0|\n",
      "|[...you, think, w...|       0.0|\n",
      "|[hold, on, a, sec...|       0.0|\n",
      "|[yeah., , they, s...|       0.0|\n",
      "|[okay,, let’s, do...|       0.0|\n",
      "|[a, lot, of, peop...|       0.0|\n",
      "|[it, belongs, to,...|       0.0|\n",
      "|[what’s, wrong, w...|       0.0|\n",
      "|[hold, that, thou...|       0.0|\n",
      "|[...you, just, bl...|       0.0|\n",
      "|[yeah., , don’t, ...|       0.0|\n",
      "|[everything’s, fu...|       0.0|\n",
      "|[no., , you’re, n...|       0.0|\n",
      "|[we’ve, got, a, h...|       0.0|\n",
      "|[one, more, stop....|       0.0|\n",
      "|[this, is, no, jo...|       0.0|\n",
      "|[this, system, ha...|       0.0|\n",
      "|[tony,, it’s, the...|       0.0|\n",
      "|[...jim,, how’re,...|       0.0|\n",
      "|[you’re, leaving,...|       0.0|\n",
      "|  [okay, --, shoot.]|       0.0|\n",
      "|[the, board, meet...|       0.0|\n",
      "|[can, i, ask, a, ...|       0.0|\n",
      "|[ridiculous., , i...|       0.0|\n",
      "|[that’s, not, bad...|       0.0|\n",
      "|[well, miss, brow...|       0.0|\n",
      "|[every, night, in...|       0.0|\n",
      "|[here’s, serious:...|       0.0|\n",
      "|[my, father, help...|       0.0|\n",
      "|[tell, me:, do, y...|       0.0|\n",
      "|[don’t, worry,, t...|       0.0|\n",
      "|[cab’s, waiting, ...|       0.0|\n",
      "|[should, i, tell,...|       0.0|\n",
      "|[five?, , i’ll, n...|       0.0|\n",
      "|[focus., , i, nee...|       0.0|\n",
      "|[you’re, rushing,...|       0.0|\n",
      "|[the, mit, commen...|       0.0|\n",
      "|[maybe., , tell, ...|       0.0|\n",
      "|[i’ll, tell, them...|       0.0|\n",
      "|[what’s, it, look...|       0.0|\n",
      "|[it’s, a, minor, ...|       0.0|\n",
      "|          [buy, it.]|       0.0|\n",
      "|[he, left, an, ho...|       0.0|\n",
      "|[you, have, plans...|       0.0|\n",
      "|[i’m, allowed, to...|       0.0|\n",
      "|[it’s, your, birt...|       0.0|\n",
      "|[yep., , funny,, ...|       0.0|\n",
      "|[well,, get, your...|       0.0|\n",
      "|     [already, did.]|       0.0|\n",
      "|           [and...?]|       0.0|\n",
      "|[it, was, very, t...|       0.0|\n",
      "|[you’re, welcome,...|       0.0|\n",
      "|[i, was, standing...|       0.0|\n",
      "|[i, had, car, tro...|       0.0|\n",
      "|[two, fingers, of...|       0.0|\n",
      "|   [we’re, working.]|       0.0|\n",
      "|[you, should, hav...|       0.0|\n",
      "|[it’s, two, in, t...|       0.0|\n",
      "|[it’s, two, in, t...|       0.0|\n",
      "|[don’t, start, wi...|       0.0|\n",
      "|[jeez,, we’re, no...|       0.0|\n",
      "|[you, don’t, get,...|       1.0|\n",
      "|[all, i, said, wa...|       0.0|\n",
      "|[‘straight-jacket...|       0.0|\n",
      "|[see, that, one?,...|       0.0|\n",
      "|[you’re, not, lis...|       0.0|\n",
      "|[i, am, listening...|       0.0|\n",
      "|[something’s...se...|       0.0|\n",
      "|[drink, 2:, a, hi...|       0.0|\n",
      "|[you, know,, hell...|       0.0|\n",
      "|[go, hang, with, ...|       1.0|\n",
      "|[i, will., that’s...|       0.0|\n",
      "|[you, could, tell...|       1.0|\n",
      "|[i, think, we, go...|       0.0|\n",
      "|[hey,, why, aren’...|       0.0|\n",
      "|[all, right,, who...|       0.0|\n",
      "|[sorry,, rhodey,,...|       0.0|\n",
      "|[what, have, you,...|       0.0|\n",
      "|[i, don’t, rememb...|       0.0|\n",
      "|[where, are, we, ...|       0.0|\n",
      "|     [...i, refuse.]|       0.0|\n",
      "|[how, did, they, ...|       0.0|\n",
      "|     [no, he, won’t]|       0.0|\n",
      "|[something’s, not...|       0.0|\n",
      "|[sir,, i’m, telli...|       0.0|\n",
      "|[with, your, perm...|       1.0|\n",
      "|[tony, stark, is,...|       0.0|\n",
      "|[why, should, i, ...|       0.0|\n",
      "|[okay,, here’s, w...|       0.0|\n",
      "|[...this, area, f...|       0.0|\n",
      "|[finally,, i, wan...|       0.0|\n",
      "|[must, have, ever...|       1.0|\n",
      "|[i, know,, they’r...|       0.0|\n",
      "|[heat, the, palla...|       0.0|\n",
      "|[the, palladium, ...|       0.0|\n",
      "|[oh, yeah,, thank...|       0.0|\n",
      "|[nice, to, meet, ...|       0.0|\n",
      "|[a, better, mouse...|       0.0|\n",
      "|[that’s, because,...|       0.0|\n",
      "|[yeah,, but, this...|       0.0|\n",
      "|[three, gigajoule...|       0.0|\n",
      "|[or, something, v...|       0.0|\n",
      "|      [a, little...]|       0.0|\n",
      "|[hey,, hey...we’v...|       0.0|\n",
      "|[looks, to, me, l...|       1.0|\n",
      "|[is, that, where,...|       0.0|\n",
      "|[“a”, i, don’t, k...|       0.0|\n",
      "|[please, don’t, u...|       0.0|\n",
      "|[unless, they’re,...|       0.0|\n",
      "|[so, that’s, it?,...|       0.0|\n",
      "|[there’s, nothing...|       0.0|\n",
      "|[spare, me., , i,...|       0.0|\n",
      "|[what, do, you, w...|       0.0|\n",
      "|[be, a, better, f...|       0.0|\n",
      "|[going, back, the...|       0.0|\n",
      "|[are, you, blocki...|       0.0|\n",
      "|      [i, am,, sir.]|       0.0|\n",
      "|[we’re, ready., ,...|       0.0|\n",
      "|[right, back, at,...|       0.0|\n",
      "|[do, you, have, a...|       0.0|\n",
      "|            [...no.]|       0.0|\n",
      "|[yeah-yeah,, enjo...|       0.0|\n",
      "|[it’s, frozen,, t...|       0.0|\n",
      "|[get, to, your, c...|       0.0|\n",
      "|[we, could’ve, ma...|       0.0|\n",
      "|[saving, your, as...|       0.0|\n",
      "|[help, me, out, o...|       0.0|\n",
      "|[i, got, you,, pal.]|       0.0|\n",
      "|       [thank, you.]|       0.0|\n",
      "|[your, eyes, are,...|       0.0|\n",
      "|[tears, of, joy.,...|       0.0|\n",
      "|[you, do, somethi...|       0.0|\n",
      "|[we’re, due, at, ...|       0.0|\n",
      "|[no, --, to, the,...|       0.0|\n",
      "|[you’ll, see., , ...|       0.0|\n",
      "|[you’ll, have, to...|       0.0|\n",
      "|[that’s, a, mouth...|       0.0|\n",
      "|[look,, mr., coul...|       0.0|\n",
      "|[well,, great,, i...|       0.0|\n",
      "|[i’m, sure, he, w...|       0.0|\n",
      "|[i...can’t, do, t...|       0.0|\n",
      "|[no,, i, don’t, w...|       0.0|\n",
      "|[yes., , that’s, ...|       0.0|\n",
      "|[what, happened, ...|       0.0|\n",
      "|[uhh,, weren’t, w...|       0.0|\n",
      "|[the, system, is,...|       0.0|\n",
      "|[in, the, coming,...|       0.0|\n",
      "|[you, mean, that?...|       0.0|\n",
      "|   [wait, and, see.]|       0.0|\n",
      "|[i, don’t, want, ...|       0.0|\n",
      "|[we, could, devel...|       0.0|\n",
      "|    [no, it, isn’t.]|       0.0|\n",
      "|[yes., , thanks, ...|       0.0|\n",
      "|   [hello,, jarvis.]|       0.0|\n",
      "|[...i, need, to, ...|       0.0|\n",
      "|[give, me, a, sca...|       0.0|\n",
      "|[it, powers, an, ...|       0.0|\n",
      "|[what, are, you, ...|       0.0|\n",
      "|[upgrade, recomme...|       0.0|\n",
      "|[because, you, ar...|       1.0|\n",
      "|[no., , it’s, ok....|       0.0|\n",
      "|[were, you, alway...|       0.0|\n",
      "|[fine., , could, ...|       0.0|\n",
      "|[ahh, that’s, mor...|       1.0|\n",
      "|    [machine, away.]|       0.0|\n",
      "|[yes., , i, remem...|       0.0|\n",
      "|[i, know, this, i...|       0.0|\n",
      "|        [not, sure.]|       0.0|\n",
      "|[pepper?, , how, ...|       0.0|\n",
      "|[agent, coulson,,...|       0.0|\n",
      "|[how, big, are, y...|       0.0|\n",
      "| [i, don’t, under--]|       0.0|\n",
      "|[--, just, get, d...|       0.0|\n",
      "|             [what?]|       0.0|\n",
      "|[just, show, me, ...|       0.0|\n",
      "|[so, that’s, the,...|       0.0|\n",
      "|[that’s, the, thi...|       0.0|\n",
      "|          [amazing.]|       0.0|\n",
      "|[i’m, going, to, ...|       0.0|\n",
      "|     [is, it, safe?]|       0.0|\n",
      "|[completely., , f...|       0.0|\n",
      "|[reach, in, to, w...|       0.0|\n",
      "|      [the, socket.]|       0.0|\n",
      "|     [what, socket?]|       0.0|\n",
      "|[the, chest, sock...|       0.0|\n",
      "|   [or, else, what?]|       0.0|\n",
      "|[i, can, go, into...|       0.0|\n",
      "|[i, thought, you,...|       0.0|\n",
      "|[i, didn’t, want,...|       0.0|\n",
      "|    [oh, my, god...]|       0.0|\n",
      "|[stay, with, me.,...|       0.0|\n",
      "|[i, don’t, know, ...|       0.0|\n",
      "|[i’m, telling, you.]|       0.0|\n",
      "|          [sorry...]|       0.0|\n",
      "|[listen., , i’m, ...|       0.0|\n",
      "|[won’t, that, mak...|       0.0|\n",
      "|[not, immediately...|       0.0|\n",
      "|   [to, the, right.]|       0.0|\n",
      "|[to, my, right., ...|       0.0|\n",
      "|    [to, the, left.]|       0.0|\n",
      "|            [right.]|       0.0|\n",
      "|             [left.]|       0.0|\n",
      "|   [right., , left.]|       0.0|\n",
      "|[how, deep, does,...|       0.0|\n",
      "|[keep, going., th...|       0.0|\n",
      "|       [ew!!!, pus!]|       0.0|\n",
      "|[it’s, not, pus.,...|       0.0|\n",
      "|[well, it, smells...|       0.0|\n",
      "| [yes., thank, you.]|       0.0|\n",
      "|[can, i, wash, my...|       0.0|\n",
      "|[the, new, unit, ...|       0.0|\n",
      "|[good,, cause, it...|       0.0|\n",
      "|      [it, is, now.]|       0.0|\n",
      "|[i, don’t, suppos...|       0.0|\n",
      "|[can, it, at, lea...|       0.0|\n",
      "|       [i, suppose.]|       0.0|\n",
      "|[throw, that, thi...|       0.0|\n",
      "|[don’t, you, want...|       0.0|\n",
      "|[why?, , it’s, an...|       0.0|\n",
      "|[you, made, it, o...|       0.0|\n",
      "|[pepper., , i, ha...|       0.0|\n",
      "|[you’re, welcome....|       0.0|\n",
      "|            [shoot.]|       0.0|\n",
      "|[i, don’t, do, we...|       1.0|\n",
      "|[i, don’t, have, ...|       0.0|\n",
      "|[will, that, be, ...|       0.0|\n",
      "|[that, will, be, ...|       0.0|\n",
      "|[these, aren’t, f...|       0.0|\n",
      "|[we’ll, start, of...|       0.0|\n",
      "|[great., , i,, uh...|       0.0|\n",
      "|[manned, or, unma...|       0.0|\n",
      "|[why, not, take, ...|       0.0|\n",
      "|[that, i’d, like,...|       1.0|\n",
      "|[who, wants, to, ...|       0.0|\n",
      "|[all, right, --, ...|       0.0|\n",
      "|         [why, not?]|       0.0|\n",
      "|[figured, you’d, ...|       0.0|\n",
      "|[why, does, every...|       0.0|\n",
      "|[you’ve, been, th...|       0.0|\n",
      "|[i’ve, got, it, s...|       0.0|\n",
      "|           [really?]|       0.0|\n",
      "|[i’m, onto, somet...|       0.0|\n",
      "|[lot, of, people,...|       0.0|\n",
      "|[i, mean, what, i...|       0.0|\n",
      "|[no, you, don’t.,...|       0.0|\n",
      "|[maybe, i, do, ne...|       0.0|\n",
      "|[all, right, then...|       0.0|\n",
      "|         [likewise.]|       0.0|\n",
      "|[thought, you, we...|       0.0|\n",
      "|[it’s, a, flight-...|       0.0|\n",
      "|[well,, watch, wh...|       0.0|\n",
      "|[be, right, up., ...|       0.0|\n",
      "|[you, know,, i, h...|       1.0|\n",
      "|[i, know,, i’m, s...|       0.0|\n",
      "|             [what?]|       0.0|\n",
      "|[how, the, hell, ...|       0.0|\n",
      "|[with, the, amoun...|       0.0|\n",
      "|[nothing, to, it....|       0.0|\n",
      "|[we’ll, do, them,...|       0.0|\n",
      "|[that’s, why, you...|       0.0|\n",
      "|[never, interrupt...|       0.0|\n",
      "|[whatever, floats...|       0.0|\n",
      "|[uhh,, jarvis?, ,...|       0.0|\n",
      "|[jarvis,, i, thin...|       0.0|\n",
      "|[you’re, a, downe...|       0.0|\n",
      "|[no,, i, got, it,...|       0.0|\n",
      "|[yeah,, and, it, ...|       0.0|\n",
      "|[re-configure, us...|       0.0|\n",
      "|[wow, me., hm., ,...|       0.0|\n",
      "|[good., , i, shou...|       0.0|\n",
      "|[it’s, time, to, ...|       0.0|\n",
      "|[great., see, ya,...|       0.0|\n",
      "|[oh...was, i, sup...|       0.0|\n",
      "|[yes,, you’re, ri...|       1.0|\n",
      "|[thanks., , it, w...|       0.0|\n",
      "|[i, have, great, ...|       1.0|\n",
      "|[no,, i, always, ...|       0.0|\n",
      "|[would, it, help,...|       0.0|\n",
      "|[you, wouldn’t, l...|       0.0|\n",
      "|[i’m, not, so, su...|       0.0|\n",
      "|[what’s, your, so...|       0.0|\n",
      "|             [uh...]|       0.0|\n",
      "|[119-64-5484, i’m...|       1.0|\n",
      "|[can, i, get, you...|       0.0|\n",
      "|[a, vodka, martin...|       0.0|\n",
      "|             [okay.]|       0.0|\n",
      "|[and,, tony..., ,...|       0.0|\n",
      "|[no., , you’re, n...|       0.0|\n",
      "|     [how’s, panic?]|       0.0|\n",
      "|[hey,, they, just...|       0.0|\n",
      "|[when, were, thes...|       0.0|\n",
      "|[i, didn’t, appro...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 300 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(numeric_MCU)\n",
    "predictionFinal_mcu = prediction.select(\n",
    "    \"new_line\", \"prediction\")\n",
    "predictionFinal_mcu.show(n=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0| 6187|\n",
      "|       1.0|  322|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = predictionFinal_mcu.groupBy('prediction').count()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
