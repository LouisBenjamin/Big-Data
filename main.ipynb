{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data: https://www.kaggle.com/pdunton/marvel-cinematic-universe-dialogue?select=mcu_subset.csv\n",
    "data NRC : https://www.kaggle.com/andradaolteanu/bing-nrc-afinn-lexicons?select=NRC.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover, RegexTokenizer, PCA\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from IPython.display import Image\n",
    "from pyspark.sql import SparkSession\n",
    "import IPython\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting The Infinity Stones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AKA Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![display image](https://media.giphy.com/media/3oxHQjRHcp4w9oi24M/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()\n",
    "#read_csv = spark.read.csv('data/tweets.csv', inferSchema=True, header=True)\n",
    "read_csv = spark.read.csv('data/Reddit_Data_utf8.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|       clean_comment|label|\n",
      "+--------------------+-----+\n",
      "|surprised modis i...|    1|\n",
      "|     naga downs ing |    0|\n",
      "| has been decided...|    0|\n",
      "|how difficult was...|    1|\n",
      "|yes now are going...|    0|\n",
      "|every question yo...|    0|\n",
      "|  the plot thickens |    0|\n",
      "|this compelling y...|    2|\n",
      "|video showing mh3...|    0|\n",
      "|brace yourself th...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data = read_csv.select(\"SentimentText\", col(\"Sentiment\").cast(\"Int\").alias(\"label\"))\n",
    "data = read_csv.select(\"clean_comment\", col(\"category\").cast(\"Int\").alias(\"label\")).dropna().dropDuplicates().replace(-1,2)\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 25668 rows.\n",
      "Testing data has 10871 rows.\n"
     ]
    }
   ],
   "source": [
    "split = data.randomSplit([0.7, 0.3])\n",
    "trainingData = split[0]\n",
    "testingData = split[1]\n",
    "print (\"Training data has\", split[0].count(), 'rows.')\n",
    "print (\"Testing data has\", split[1].count(), 'rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data (Tokenizing and Stop Word Removing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|       clean_comment|label|              Tokens|         NoStopWords|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "| all the signs em...|    1|[all, the, signs,...|[signs, emergency...|\n",
      "| assholery has fa...|    0|[assholery, has, ...|[assholery, face,...|\n",
      "| because fucking ...|    0|[because, fucking...|[fucking, white, ...|\n",
      "| bjp will proacti...|    2|[bjp, will, proac...|[bjp, proactive, ...|\n",
      "| breaking news ma...|    0|[breaking, news, ...|[breaking, news, ...|\n",
      "| couldn see havin...|    1|[couldn, see, hav...|[couldn, see, pos...|\n",
      "| doing decent job...|    1|[doing, decent, j...|[decent, job, com...|\n",
      "| don know ethical...|    1|[don, know, ethic...|[know, ethical, f...|\n",
      "| explained this a...|    2|[explained, this,...|[explained, anecd...|\n",
      "| feel like part r...|    0|[feel, like, part...|[feel, like, part...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|       clean_comment|label|              Tokens|         NoStopWords|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "| 5ppr for flex sp...|    0|[5ppr, for, flex,...|[5ppr, flex, spot...|\n",
      "| are you saying t...|    0|[are, you, saying...|[saying, tally, b...|\n",
      "| back vent here a...|    0|[back, vent, here...|  [back, vent, lose]|\n",
      "| big political le...|    1|[big, political, ...|[big, political, ...|\n",
      "| bunch aap suppor...|    2|[bunch, aap, supp...|[bunch, aap, supp...|\n",
      "| don see how this...|    2|[don, see, how, t...|[see, unpopular, ...|\n",
      "| feel bad for the...|    2|[feel, bad, for, ...|[feel, bad, woode...|\n",
      "| hai aam modi aap...|    2|[hai, aam, modi, ...|[hai, aam, modi, ...|\n",
      "| have similar cha...|    0|[have, similar, c...|[similar, chart, ...|\n",
      "| need bjp mukt bh...|    0|[need, bjp, mukt,...|[need, bjp, mukt,...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inputCol = \"SentimentText\"\n",
    "inputCol = \"clean_comment\"\n",
    "\n",
    "tokenizer = RegexTokenizer(pattern=r'(?:\\p{Punct}|\\s)+', inputCol=inputCol, outputCol=\"Tokens\")\n",
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"NoStopWords\")\n",
    "\n",
    "token_train = tokenizer.transform(trainingData)\n",
    "nosw_train = swr.transform(token_train)\n",
    "\n",
    "token_test = tokenizer.transform(testingData)\n",
    "nosw_test = swr.transform(token_test)\n",
    "\n",
    "nosw_train.show(truncate=True, n=10)\n",
    "nosw_test.show(truncate=True, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing The Features using HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|              Tokens|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    1|[all, the, signs,...|(262144,[2306,256...|\n",
      "|    0|[assholery, has, ...|(262144,[13644,20...|\n",
      "|    0|[because, fucking...|(262144,[41748,43...|\n",
      "|    2|[bjp, will, proac...|(262144,[1141,670...|\n",
      "|    0|[breaking, news, ...|(262144,[18981,27...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+--------------------+\n",
      "|Label|              Tokens|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[5ppr, for, flex,...|(262144,[5972,147...|\n",
      "|    0|[are, you, saying...|(262144,[160395,1...|\n",
      "|    0|[back, vent, here...|(262144,[30297,13...|\n",
      "|    1|[big, political, ...|(262144,[3657,334...|\n",
      "|    2|[bunch, aap, supp...|(262144,[13468,33...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "hash_train = hashTF.transform(nosw_train).select(\n",
    "    'label', 'Tokens', 'features')\n",
    "\n",
    "hash_test = hashTF.transform(nosw_test).select(\n",
    "    'Label', 'Tokens', 'features')\n",
    "hash_train.show(n=5)\n",
    "hash_test.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlor = (LogisticRegression()\n",
    "       .setFamily(\"multinomial\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= mlor.fit(hash_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+--------------------+--------------------+--------------------+----------+\n",
      "|Label|               Tokens|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+---------------------+--------------------+--------------------+--------------------+----------+\n",
      "|    0| [5ppr, for, flex,...|(262144,[5972,147...|[41.1129939291239...|[1.0,4.9176315886...|       0.0|\n",
      "|    0| [are, you, saying...|(262144,[160395,1...|[5.08306291156648...|[0.36750222290442...|       1.0|\n",
      "|    0| [back, vent, here...|(262144,[30297,13...|[7.78969378741204...|[0.99979152794506...|       0.0|\n",
      "|    1| [big, political, ...|(262144,[3657,334...|[2.30518583959471...|[4.53408415891803...|       1.0|\n",
      "|    2| [bunch, aap, supp...|(262144,[13468,33...|[-2.6346333542633...|[3.09661797338132...|       1.0|\n",
      "|    2| [don, see, how, t...|(262144,[8538,942...|[15.7602014236787...|[0.99999999807920...|       0.0|\n",
      "|    2| [feel, bad, for, ...|(262144,[22370,61...|[-7.4183882970740...|[6.73076261818471...|       1.0|\n",
      "|    2| [hai, aam, modi, ...|(262144,[1386,789...|[52.3210415033804...|[1.0,2.2888596935...|       0.0|\n",
      "|    0| [have, similar, c...|(262144,[8538,295...|[-4.9476829147295...|[9.05295743858447...|       2.0|\n",
      "|    0| [need, bjp, mukt,...|(262144,[83161,11...|[-1.2568928693020...|[0.02143977404386...|       1.0|\n",
      "|    0| [never, visited, ...|(262144,[60825,97...|[9.32312545220233...|[0.99999806859089...|       0.0|\n",
      "|    0| [press, conferenc...|(262144,[68649,19...|[-4.7657843989948...|[3.44028129744917...|       1.0|\n",
      "|    0| [should, finally,...|(262144,[7625,731...|[28.8041763558159...|[1.0,4.6402328911...|       0.0|\n",
      "|    0| [teacher, back, t...|(262144,[17725,53...|[4.42343239924907...|[0.99498302267895...|       0.0|\n",
      "|    1| [that, what, the,...|(262144,[40168,22...|[-12.215019200446...|[5.64468892772171...|       1.0|\n",
      "|    1| [this, doesn, mak...|(262144,[18176,25...|[-5.0503525741722...|[2.87163852241426...|       1.0|\n",
      "|    2| [true, then, bad,...|(262144,[49185,75...|[-20.106383313014...|[3.18694466689166...|       1.0|\n",
      "|    2| [unpopular, opini...|(262144,[4075,912...|[-61.246605793847...|[4.70653381067228...|       2.0|\n",
      "|    2| [was, museum, tod...|(262144,[8419,356...|[-1.4766763010679...|[0.01418574642036...|       2.0|\n",
      "|    1| [will, make, sure...|(262144,[88302,89...|[-1.7827975234978...|[2.89864296847162...|       2.0|\n",
      "|    1| [wouldn, surprise...|(262144,[6957,142...|[-22.680093756150...|[1.08748393972974...|       1.0|\n",
      "|    1| [aib, mainstream,...|(262144,[11422,16...|[-160.05987191593...|[1.42324390850868...|       1.0|\n",
      "|    0| [and, remind, zeu...|(262144,[93197,13...|[1.55364370884445...|[7.45752944300700...|       2.0|\n",
      "|    1| [apex, court, cas...|(262144,[14376,31...|[-32.164321224573...|[8.14161331765215...|       1.0|\n",
      "|    1| [best, among, all...|(262144,[113432,1...|[-7.9425121017868...|[7.55466673356757...|       1.0|\n",
      "|    2| [bhakti, any, kin...|(262144,[19191,21...|[11.6988871188123...|[0.01317589286590...|       1.0|\n",
      "|    0| [brace, yourself,...|(262144,[12409,28...|[27.5685860504467...|[1.0,2.6024895256...|       0.0|\n",
      "|    1| [but, will, vote,...|(262144,[49918,13...|[5.67744613201197...|[0.99752186567754...|       0.0|\n",
      "|    0| [college, hone, w...|(262144,[53774,68...|[16.3654403510388...|[0.99999999451835...|       0.0|\n",
      "|    1| [dude, ain, got, ...|(262144,[20719,22...|[14.0551647400854...|[1.49024312662923...|       1.0|\n",
      "|    0| [effin, pumped, t...|(262144,[45121,14...|[-5.7251531527134...|[1.50225487381132...|       1.0|\n",
      "|    1| [finally, trump, ...|(262144,[7625,371...|[-3.6719771263223...|[1.41933370763085...|       1.0|\n",
      "|    1| [fresh, maal, tak...|(262144,[41272,55...|[6.77887687138557...|[0.99333107409209...|       0.0|\n",
      "|    2| [fucking, lynch, ...|(262144,[1696,349...|[-32.434615785938...|[4.25290689077623...|       2.0|\n",
      "|    2| [germany, economi...|(262144,[1696,397...|[-13.291271229879...|[1.67356091712513...|       2.0|\n",
      "|    1| [how, difficult, ...|(262144,[96783,14...|[-5.2180971581279...|[8.42655956414339...|       2.0|\n",
      "|    0| [india, nuff, sai...|(262144,[60825,70...|[12.0923757087560...|[0.72350811770046...|       0.0|\n",
      "|    0|   [isme, modi, hath]|(262144,[3722,409...|[19.0343570695411...|[0.99999999999769...|       0.0|\n",
      "|    1| [isn, bit, early,...|(262144,[31536,88...|[-0.2990136185567...|[0.00653868991669...|       2.0|\n",
      "|    2| [itt, sacco, gent...|(262144,[26205,67...|[3.20354993850201...|[1.36512246445064...|       2.0|\n",
      "|    0| [letter, likh, bhai]|(262144,[97895,19...|[31.0143615947664...|[1.0,1.6167986775...|       0.0|\n",
      "|    0| [looks, like, the...|(262144,[24811,35...|[1.76515738878650...|[0.00586906484986...|       2.0|\n",
      "|    1| [meme, warfare, h...|(262144,[73249,12...|[10.1778413960151...|[0.99999888543896...|       0.0|\n",
      "|    0|   [naga, downs, ing]|(262144,[76478,11...|[13.2483345133239...|[0.99999996597331...|       0.0|\n",
      "|    0|             [nagant]|(262144,[92164],[...|[-10.621018608389...|[7.95674454043900...|       2.0|\n",
      "|    2| [people, need, un...|(262144,[6957,273...|[-19.175076549945...|[3.47562028141607...|       2.0|\n",
      "|    0|        [some, views]|(262144,[33783],[...|[5.36804476358751...|[0.99371383760676...|       0.0|\n",
      "|    1| [thanks, for, thi...|(262144,[64358,11...|[-13.694484412611...|[9.83804194800687...|       1.0|\n",
      "|    2| [they, are, not, ...|(262144,[6801,853...|[-59.547142546020...|[5.28112381268676...|       1.0|\n",
      "|    1| [they, have, poli...|(262144,[11995,38...|[-76.175215341432...|[3.16647541287288...|       2.0|\n",
      "|    1| [they, loved, him...|(262144,[52131,56...|[-5.0102066937809...|[1.27566052783909...|       1.0|\n",
      "|    1| [this, has, nothi...|(262144,[12336,18...|[4.20666665707071...|[0.00743693737757...|       2.0|\n",
      "|    0| [this, picture, m...|(262144,[14827,24...|[-3.1278205704252...|[2.52412197226883...|       1.0|\n",
      "|    2| [this, such, weak...|(262144,[4900,600...|[-4.0099080697194...|[1.02137087688546...|       2.0|\n",
      "|    2| [ugh, this, horri...|(262144,[99877,10...|[12.9901301257412...|[0.98072364977159...|       0.0|\n",
      "|    0| [video, showing, ...|(262144,[2284,300...|[-0.6293892236248...|[0.12104668451482...|       1.0|\n",
      "|    0|              [vikas]|(262144,[95270],[...|[10.8358371511273...|[0.99998150471525...|       0.0|\n",
      "|    0| [wait, how, mimir...|(262144,[101323,1...|[32.6587724069660...|[1.0,1.6188843872...|       0.0|\n",
      "|    1| [what, pisses, of...|(262144,[31536,41...|[-10.996324575711...|[4.06670615552004...|       1.0|\n",
      "|    2| [when, the, actua...|(262144,[21570,31...|[-18.318022285283...|[3.79497959453551...|       1.0|\n",
      "|    1| [while, completel...|(262144,[7231,144...|[-50.468919244732...|[2.75738215974531...|       1.0|\n",
      "|    1| [wow, they, just,...|(262144,[4631,499...|[-31.526608461420...|[1.66461634437811...|       1.0|\n",
      "|    1| [yeah, found, the...|(262144,[12005,77...|[-0.1672176751649...|[1.83836502164746...|       2.0|\n",
      "|    0|[（, ͡°, ͜, ͡°, ━☆...|(262144,[12180,12...|[16.2741457114383...|[0.99999999903432...|       0.0|\n",
      "|    0|          [byungkwan]|(262144,[259900],...|[5.10254434433144...|[0.99879690585973...|       0.0|\n",
      "|    2| [did, jim, carrey...|(262144,[20986,25...|[35.2620465358895...|[1.0,6.1868427797...|       0.0|\n",
      "|    2| [dolph, ziggler, ...|(262144,[8538,191...|[26.3151353304646...|[0.99999999999999...|       0.0|\n",
      "|    1| [edit, convert, y...|(262144,[16004,25...|[23.7315997315242...|[0.99999999999999...|       0.0|\n",
      "|    1| [hearing, these, ...|(262144,[3928,651...|[-79.825511895700...|[9.46671758015048...|       1.0|\n",
      "|    1| [indeed, surprise...|(262144,[12987,36...|[-11.106248076943...|[8.42647892260903...|       1.0|\n",
      "|    1|        [own, niggas]|(262144,[105482],...|[-13.332432823028...|[7.46488354414500...|       1.0|\n",
      "|    0|         [pole, star]|(262144,[213314,2...|[19.1990279807523...|[0.99999999998898...|       0.0|\n",
      "|    1| [shared, this, ar...|(262144,[18176,59...|[-5.4232378278005...|[8.05885691667146...|       1.0|\n",
      "|    0| [soccorritore, av...|(262144,[6795,922...|[5.77546982925377...|[0.99882344848556...|       0.0|\n",
      "|    0| [something, final...|(262144,[7625,570...|[0.62334785816214...|[7.71013567542804...|       1.0|\n",
      "|    2| [tejasvi, haripra...|(262144,[8748,218...|[11.4836847081192...|[0.99985928709863...|       0.0|\n",
      "|    1| [they, took, bill...|(262144,[21823,32...|[-2.7974136737702...|[1.23871432553576...|       1.0|\n",
      "|    1| [think, healthier...|(262144,[2306,270...|[-106.09379197985...|[2.43400182471753...|       1.0|\n",
      "|    1| [think, ’, many, ...|(262144,[9781,107...|[-53.225864346849...|[1.26461426233760...|       2.0|\n",
      "|    0| [this, lie, bajar...|(262144,[12090,10...|[0.66064013700712...|[0.05222582359664...|       1.0|\n",
      "|    2| [very, sad, that,...|(262144,[22962,35...|[-18.257306543889...|[6.81746935910973...|       1.0|\n",
      "|    2| [was, expecting, ...|(262144,[97894,18...|[5.03054536369545...|[0.01777556750571...|       2.0|\n",
      "|    0| [all, were, kille...|(262144,[73495,16...|[10.4876766017805...|[0.99825845976025...|       0.0|\n",
      "|    2| [anti, national, ...|(262144,[2306,720...|[40.0418939298954...|[0.99999999999999...|       0.0|\n",
      "|    0| [are, there, any,...|(262144,[49185,13...|[1.53921972628868...|[0.56829350714813...|       0.0|\n",
      "|    2| [because, muslims...|(262144,[33012,35...|[-13.331301755979...|[3.11614600445875...|       2.0|\n",
      "|    2| [bjp, put, all, i...|(262144,[21823,40...|[-11.433458364850...|[1.56955720019343...|       1.0|\n",
      "|    0| [can, someone, an...|(262144,[4075,121...|[16.5458885308099...|[0.99999999912554...|       0.0|\n",
      "|    2| [congress, never,...|(262144,[2306,417...|[-3.6715839101959...|[3.19585004743860...|       1.0|\n",
      "|    2| [dae, find, cring...|(262144,[2437,856...|[-27.520441955069...|[2.75808775012670...|       2.0|\n",
      "|    0|      [danerys, dies]|(262144,[172575,2...|[10.0080367534961...|[0.99999676828924...|       0.0|\n",
      "|    1| [feeling, proud, ...|(262144,[137733,2...|[-10.519794589862...|[5.31931432068935...|       1.0|\n",
      "|    2| [how, effective, ...|(262144,[17491,21...|[-74.602349329923...|[8.25819399135241...|       2.0|\n",
      "|    1| [just, expected, ...|(262144,[440,1379...|[-84.906375761688...|[2.03563822668000...|       1.0|\n",
      "|    0| [kaha, milega, it...|(262144,[23418,12...|[24.8511102110719...|[0.99999999993717...|       0.0|\n",
      "|    0| [lynching, rate, ...|(262144,[14385,68...|[9.74495904608503...|[0.99999891221829...|       0.0|\n",
      "|    0| [mukkhi, couldn, ...|(262144,[40299,79...|[15.4329292841240...|[0.99995584866967...|       0.0|\n",
      "|    1| [now, have, watch...|(262144,[23837,57...|[-12.463021744038...|[4.17208155237600...|       1.0|\n",
      "|    0| [patriotism, the,...|(262144,[5381,407...|[20.0377321122839...|[0.99999999935341...|       0.0|\n",
      "|    2| [please, rebel, y...|(262144,[56627,64...|[2.28267725512468...|[0.01731356420021...|       2.0|\n",
      "+-----+---------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(hash_test)\n",
    "prediction.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------+-----+\n",
      "|               Tokens|prediction|Label|\n",
      "+---------------------+----------+-----+\n",
      "| [5ppr, for, flex,...|       0.0|    0|\n",
      "| [are, you, saying...|       1.0|    0|\n",
      "| [back, vent, here...|       0.0|    0|\n",
      "| [big, political, ...|       1.0|    1|\n",
      "| [bunch, aap, supp...|       1.0|    2|\n",
      "| [don, see, how, t...|       0.0|    2|\n",
      "| [feel, bad, for, ...|       1.0|    2|\n",
      "| [hai, aam, modi, ...|       0.0|    2|\n",
      "| [have, similar, c...|       2.0|    0|\n",
      "| [need, bjp, mukt,...|       1.0|    0|\n",
      "| [never, visited, ...|       0.0|    0|\n",
      "| [press, conferenc...|       1.0|    0|\n",
      "| [should, finally,...|       0.0|    0|\n",
      "| [teacher, back, t...|       0.0|    0|\n",
      "| [that, what, the,...|       1.0|    1|\n",
      "| [this, doesn, mak...|       1.0|    1|\n",
      "| [true, then, bad,...|       1.0|    2|\n",
      "| [unpopular, opini...|       2.0|    2|\n",
      "| [was, museum, tod...|       2.0|    2|\n",
      "| [will, make, sure...|       2.0|    1|\n",
      "| [wouldn, surprise...|       1.0|    1|\n",
      "| [aib, mainstream,...|       1.0|    1|\n",
      "| [and, remind, zeu...|       2.0|    0|\n",
      "| [apex, court, cas...|       1.0|    1|\n",
      "| [best, among, all...|       1.0|    1|\n",
      "| [bhakti, any, kin...|       1.0|    2|\n",
      "| [brace, yourself,...|       0.0|    0|\n",
      "| [but, will, vote,...|       0.0|    1|\n",
      "| [college, hone, w...|       0.0|    0|\n",
      "| [dude, ain, got, ...|       1.0|    1|\n",
      "| [effin, pumped, t...|       1.0|    0|\n",
      "| [finally, trump, ...|       1.0|    1|\n",
      "| [fresh, maal, tak...|       0.0|    1|\n",
      "| [fucking, lynch, ...|       2.0|    2|\n",
      "| [germany, economi...|       2.0|    2|\n",
      "| [how, difficult, ...|       2.0|    1|\n",
      "| [india, nuff, sai...|       0.0|    0|\n",
      "|   [isme, modi, hath]|       0.0|    0|\n",
      "| [isn, bit, early,...|       2.0|    1|\n",
      "| [itt, sacco, gent...|       2.0|    2|\n",
      "| [letter, likh, bhai]|       0.0|    0|\n",
      "| [looks, like, the...|       2.0|    0|\n",
      "| [meme, warfare, h...|       0.0|    1|\n",
      "|   [naga, downs, ing]|       0.0|    0|\n",
      "|             [nagant]|       2.0|    0|\n",
      "| [people, need, un...|       2.0|    2|\n",
      "|        [some, views]|       0.0|    0|\n",
      "| [thanks, for, thi...|       1.0|    1|\n",
      "| [they, are, not, ...|       1.0|    2|\n",
      "| [they, have, poli...|       2.0|    1|\n",
      "| [they, loved, him...|       1.0|    1|\n",
      "| [this, has, nothi...|       2.0|    1|\n",
      "| [this, picture, m...|       1.0|    0|\n",
      "| [this, such, weak...|       2.0|    2|\n",
      "| [ugh, this, horri...|       0.0|    2|\n",
      "| [video, showing, ...|       1.0|    0|\n",
      "|              [vikas]|       0.0|    0|\n",
      "| [wait, how, mimir...|       0.0|    0|\n",
      "| [what, pisses, of...|       1.0|    1|\n",
      "| [when, the, actua...|       1.0|    2|\n",
      "| [while, completel...|       1.0|    1|\n",
      "| [wow, they, just,...|       1.0|    1|\n",
      "| [yeah, found, the...|       2.0|    1|\n",
      "|[（, ͡°, ͜, ͡°, ━☆...|       0.0|    0|\n",
      "|          [byungkwan]|       0.0|    0|\n",
      "| [did, jim, carrey...|       0.0|    2|\n",
      "| [dolph, ziggler, ...|       0.0|    2|\n",
      "| [edit, convert, y...|       0.0|    1|\n",
      "| [hearing, these, ...|       1.0|    1|\n",
      "| [indeed, surprise...|       1.0|    1|\n",
      "|        [own, niggas]|       1.0|    1|\n",
      "|         [pole, star]|       0.0|    0|\n",
      "| [shared, this, ar...|       1.0|    1|\n",
      "| [soccorritore, av...|       0.0|    0|\n",
      "| [something, final...|       1.0|    0|\n",
      "| [tejasvi, haripra...|       0.0|    2|\n",
      "| [they, took, bill...|       1.0|    1|\n",
      "| [think, healthier...|       1.0|    1|\n",
      "| [think, ’, many, ...|       2.0|    1|\n",
      "| [this, lie, bajar...|       1.0|    0|\n",
      "| [very, sad, that,...|       1.0|    2|\n",
      "| [was, expecting, ...|       2.0|    2|\n",
      "| [all, were, kille...|       0.0|    0|\n",
      "| [anti, national, ...|       0.0|    2|\n",
      "| [are, there, any,...|       0.0|    0|\n",
      "| [because, muslims...|       2.0|    2|\n",
      "| [bjp, put, all, i...|       1.0|    2|\n",
      "| [can, someone, an...|       0.0|    0|\n",
      "| [congress, never,...|       1.0|    2|\n",
      "| [dae, find, cring...|       2.0|    2|\n",
      "|      [danerys, dies]|       0.0|    0|\n",
      "| [feeling, proud, ...|       1.0|    1|\n",
      "| [how, effective, ...|       2.0|    2|\n",
      "| [just, expected, ...|       1.0|    1|\n",
      "| [kaha, milega, it...|       0.0|    0|\n",
      "| [lynching, rate, ...|       0.0|    0|\n",
      "| [mukkhi, couldn, ...|       0.0|    0|\n",
      "| [now, have, watch...|       1.0|    1|\n",
      "| [patriotism, the,...|       0.0|    0|\n",
      "| [please, rebel, y...|       2.0|    2|\n",
      "+---------------------+----------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionFinal = prediction.select(\n",
    "    \"Tokens\", \"prediction\", \"Label\")\n",
    "predictionFinal.show(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6787784012510348\n"
     ]
    }
   ],
   "source": [
    "match = predictionFinal.filter(predictionFinal['prediction'] == predictionFinal['label']).count()\n",
    "total = predictionFinal.count()\n",
    "print(\"Accuracy:\", match/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = predictionFinal.drop('Tokens')\n",
    "\n",
    "# predictionAndLabels = temp.rdd.map(lambda lp: (float(prediciton), lp.label))\n",
    "predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 830.0 failed 1 times, most recent failure: Lost task 0.0 in stage 830.0 (TID 29574, DESKTOP-CTIMH20, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\Ben\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\Users\\Ben\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 597, in process\n  File \"C:\\Users\\Ben\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\Ben\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 1440, in takeUpToNumLeft\n    yield next(iterator)\n  File \"C:\\Users\\Ben\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-75-9f8004798c20>\", line 3, in <lambda>\nNameError: name 'prediciton' is not defined\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\Ben\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\Users\\Ben\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 597, in process\n  File \"C:\\Users\\Ben\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\Ben\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 1440, in takeUpToNumLeft\n    yield next(iterator)\n  File \"C:\\Users\\Ben\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-75-9f8004798c20>\", line 3, in <lambda>\nNameError: name 'prediciton' is not defined\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-cce319c7befe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMulticlassMetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictionAndLabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Overall statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\mllib\\evaluation.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, predictionAndLabels)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictionAndLabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0msql_ctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[0mnumCol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictionAndLabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m         schema = StructType([\n\u001b[0;32m    258\u001b[0m             \u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"prediction\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDoubleType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnullable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1462\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m         \"\"\"\n\u001b[1;32m-> 1464\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1465\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1446\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1448\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1116\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1118\u001b[1;33m         \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1119\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 830.0 failed 1 times, most recent failure: Lost task 0.0 in stage 830.0 (TID 29574, DESKTOP-CTIMH20, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\Ben\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\Users\\Ben\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 597, in process\n  File \"C:\\Users\\Ben\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\Ben\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 1440, in takeUpToNumLeft\n    yield next(iterator)\n  File \"C:\\Users\\Ben\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-75-9f8004798c20>\", line 3, in <lambda>\nNameError: name 'prediciton' is not defined\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\Ben\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"C:\\Users\\Ben\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 597, in process\n  File \"C:\\Users\\Ben\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 271, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\Ben\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 1440, in takeUpToNumLeft\n    yield next(iterator)\n  File \"C:\\Users\\Ben\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-75-9f8004798c20>\", line 3, in <lambda>\nNameError: name 'prediciton' is not defined\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "# Overall statistics\n",
    "precision = metrics.precision(1.0)\n",
    "recall = metrics.recall(1.0)\n",
    "f1Score = metrics.fMeasure(1.0)\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avengers Assemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![display image](https://media.giphy.com/media/j2pWZpr5RlpCodOB0d/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines of Dialogue: 6509\n"
     ]
    }
   ],
   "source": [
    "mcu_csv = spark.read.csv('data/mcu_subset.csv', inferSchema=True, header=True)\n",
    "print(\"Lines of Dialogue:\", mcu_csv.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|   character|                line|\n",
      "+------------+--------------------+\n",
      "|  TONY STARK|Oh, I get it.  Yo...|\n",
      "|  TONY STARK|Oh.  I see.  So i...|\n",
      "|  TONY STARK|Good God, you’re ...|\n",
      "|  TONY STARK|             Please.|\n",
      "|  TONY STARK|Excellent questio...|\n",
      "|  TONY STARK|      Join the club.|\n",
      "|  TONY STARK|Are you aware tha...|\n",
      "|JAMES RHODES|GET DOWN, TONY.  ...|\n",
      "|JAMES RHODES|As Program Manage...|\n",
      "|  TONY STARK|...you think we’r...|\n",
      "+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = mcu_csv.select(\"character\",\"line\")\n",
    "data.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+\n",
      "|   character|                line|            new_line|                 new|\n",
      "+------------+--------------------+--------------------+--------------------+\n",
      "|  TONY STARK|Oh, I get it.  Yo...|[oh,, i, get, it....|[oh,, get, it., ,...|\n",
      "|  TONY STARK|Oh.  I see.  So i...|[oh., , i, see., ...|[oh., , see., , i...|\n",
      "|  TONY STARK|Good God, you’re ...|[good, god,, you’...|[good, god,, you’...|\n",
      "|  TONY STARK|             Please.|           [please.]|           [please.]|\n",
      "|  TONY STARK|Excellent questio...|[excellent, quest...|[excellent, quest...|\n",
      "|  TONY STARK|      Join the club.|  [join, the, club.]|       [join, club.]|\n",
      "|  TONY STARK|Are you aware tha...|[are, you, aware,...|[aware, native, a...|\n",
      "|JAMES RHODES|GET DOWN, TONY.  ...|[get, down,, tony...|[get, down,, tony...|\n",
      "|JAMES RHODES|As Program Manage...|[as, program, man...|[program, manager...|\n",
      "|  TONY STARK|...you think we’r...|[...you, think, w...|[...you, think, w...|\n",
      "+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer(inputCol=\"line\", outputCol=\"new_line\")\n",
    "swr_MCU = StopWordsRemover(inputCol=t.getOutputCol(), \n",
    "                       outputCol=\"new\")\n",
    "token_MCU = t.transform(data)\n",
    "nosw_MCU = swr_MCU.transform(token_MCU)\n",
    "\n",
    "nosw_MCU.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            new_line|            features|\n",
      "+--------------------+--------------------+\n",
      "|[oh,, i, get, it....|(262144,[44954,84...|\n",
      "|[oh., , i, see., ...|(262144,[8938,109...|\n",
      "|[good, god,, you’...|(262144,[6808,353...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashTF = HashingTF(inputCol=swr_MCU.getOutputCol(), outputCol=\"features\")\n",
    "hash_MCU = hashTF.transform(nosw_MCU).select('new_line', 'features')\n",
    "hash_MCU.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            new_line|prediction|\n",
      "+--------------------+----------+\n",
      "|[oh,, i, get, it....|       0.0|\n",
      "|[oh., , i, see., ...|       0.0|\n",
      "|[good, god,, you’...|       0.0|\n",
      "|           [please.]|       2.0|\n",
      "|[excellent, quest...|       2.0|\n",
      "|  [join, the, club.]|       0.0|\n",
      "|[are, you, aware,...|       0.0|\n",
      "|[get, down,, tony...|       0.0|\n",
      "|[as, program, man...|       0.0|\n",
      "|[...you, think, w...|       1.0|\n",
      "|[hold, on, a, sec...|       2.0|\n",
      "|[yeah., , they, s...|       1.0|\n",
      "|[okay,, let’s, do...|       1.0|\n",
      "|[a, lot, of, peop...|       0.0|\n",
      "|[it, belongs, to,...|       0.0|\n",
      "|[what’s, wrong, w...|       0.0|\n",
      "|[hold, that, thou...|       0.0|\n",
      "|[...you, just, bl...|       2.0|\n",
      "|[yeah., , don’t, ...|       1.0|\n",
      "|[everything’s, fu...|       1.0|\n",
      "|[no., , you’re, n...|       0.0|\n",
      "|[we’ve, got, a, h...|       0.0|\n",
      "|[one, more, stop....|       0.0|\n",
      "|[this, is, no, jo...|       1.0|\n",
      "|[this, system, ha...|       1.0|\n",
      "|[tony,, it’s, the...|       1.0|\n",
      "|[...jim,, how’re,...|       1.0|\n",
      "|[you’re, leaving,...|       1.0|\n",
      "|  [okay, --, shoot.]|       1.0|\n",
      "|[the, board, meet...|       0.0|\n",
      "|[can, i, ask, a, ...|       1.0|\n",
      "|[ridiculous., , i...|       0.0|\n",
      "|[that’s, not, bad...|       2.0|\n",
      "|[well, miss, brow...|       1.0|\n",
      "|[every, night, in...|       0.0|\n",
      "|[here’s, serious:...|       1.0|\n",
      "|[my, father, help...|       0.0|\n",
      "|[tell, me:, do, y...|       0.0|\n",
      "|[don’t, worry,, t...|       0.0|\n",
      "|[cab’s, waiting, ...|       1.0|\n",
      "|[should, i, tell,...|       0.0|\n",
      "|[five?, , i’ll, n...|       0.0|\n",
      "|[focus., , i, nee...|       0.0|\n",
      "|[you’re, rushing,...|       0.0|\n",
      "|[the, mit, commen...|       0.0|\n",
      "|[maybe., , tell, ...|       0.0|\n",
      "|[i’ll, tell, them...|       0.0|\n",
      "|[what’s, it, look...|       0.0|\n",
      "|[it’s, a, minor, ...|       2.0|\n",
      "|          [buy, it.]|       0.0|\n",
      "|[he, left, an, ho...|       0.0|\n",
      "|[you, have, plans...|       0.0|\n",
      "|[i’m, allowed, to...|       0.0|\n",
      "|[it’s, your, birt...|       0.0|\n",
      "|[yep., , funny,, ...|       0.0|\n",
      "|[well,, get, your...|       0.0|\n",
      "|     [already, did.]|       0.0|\n",
      "|           [and...?]|       0.0|\n",
      "|[it, was, very, t...|       0.0|\n",
      "|[you’re, welcome,...|       0.0|\n",
      "|[i, was, standing...|       0.0|\n",
      "|[i, had, car, tro...|       0.0|\n",
      "|[two, fingers, of...|       0.0|\n",
      "|   [we’re, working.]|       0.0|\n",
      "|[you, should, hav...|       0.0|\n",
      "|[it’s, two, in, t...|       0.0|\n",
      "|[it’s, two, in, t...|       1.0|\n",
      "|[don’t, start, wi...|       0.0|\n",
      "|[jeez,, we’re, no...|       1.0|\n",
      "|[you, don’t, get,...|       0.0|\n",
      "|[all, i, said, wa...|       1.0|\n",
      "|[‘straight-jacket...|       0.0|\n",
      "|[see, that, one?,...|       0.0|\n",
      "|[you’re, not, lis...|       0.0|\n",
      "|[i, am, listening...|       0.0|\n",
      "|[something’s...se...|       2.0|\n",
      "|[drink, 2:, a, hi...|       1.0|\n",
      "|[you, know,, hell...|       0.0|\n",
      "|[go, hang, with, ...|       1.0|\n",
      "|[i, will., that’s...|       0.0|\n",
      "|[you, could, tell...|       1.0|\n",
      "|[i, think, we, go...|       0.0|\n",
      "|[hey,, why, aren’...|       0.0|\n",
      "|[all, right,, who...|       1.0|\n",
      "|[sorry,, rhodey,,...|       2.0|\n",
      "|[what, have, you,...|       0.0|\n",
      "|[i, don’t, rememb...|       0.0|\n",
      "|[where, are, we, ...|       0.0|\n",
      "|     [...i, refuse.]|       2.0|\n",
      "|[how, did, they, ...|       0.0|\n",
      "|     [no, he, won’t]|       0.0|\n",
      "|[something’s, not...|       0.0|\n",
      "|[sir,, i’m, telli...|       2.0|\n",
      "|[with, your, perm...|       0.0|\n",
      "|[tony, stark, is,...|       1.0|\n",
      "|[why, should, i, ...|       2.0|\n",
      "|[okay,, here’s, w...|       2.0|\n",
      "|[...this, area, f...|       1.0|\n",
      "|[finally,, i, wan...|       1.0|\n",
      "|[must, have, ever...|       1.0|\n",
      "|[i, know,, they’r...|       2.0|\n",
      "|[heat, the, palla...|       2.0|\n",
      "|[the, palladium, ...|       0.0|\n",
      "|[oh, yeah,, thank...|       0.0|\n",
      "|[nice, to, meet, ...|       1.0|\n",
      "|[a, better, mouse...|       1.0|\n",
      "|[that’s, because,...|       0.0|\n",
      "|[yeah,, but, this...|       0.0|\n",
      "|[three, gigajoule...|       0.0|\n",
      "|[or, something, v...|       0.0|\n",
      "|      [a, little...]|       0.0|\n",
      "|[hey,, hey...we’v...|       0.0|\n",
      "|[looks, to, me, l...|       0.0|\n",
      "|[is, that, where,...|       0.0|\n",
      "|[“a”, i, don’t, k...|       0.0|\n",
      "|[please, don’t, u...|       0.0|\n",
      "|[unless, they’re,...|       2.0|\n",
      "|[so, that’s, it?,...|       2.0|\n",
      "|[there’s, nothing...|       0.0|\n",
      "|[spare, me., , i,...|       0.0|\n",
      "|[what, do, you, w...|       0.0|\n",
      "|[be, a, better, f...|       1.0|\n",
      "|[going, back, the...|       0.0|\n",
      "|[are, you, blocki...|       0.0|\n",
      "|      [i, am,, sir.]|       2.0|\n",
      "|[we’re, ready., ,...|       0.0|\n",
      "|[right, back, at,...|       1.0|\n",
      "|[do, you, have, a...|       0.0|\n",
      "|            [...no.]|       0.0|\n",
      "|[yeah-yeah,, enjo...|       1.0|\n",
      "|[it’s, frozen,, t...|       0.0|\n",
      "|[get, to, your, c...|       1.0|\n",
      "|[we, could’ve, ma...|       0.0|\n",
      "|[saving, your, as...|       2.0|\n",
      "|[help, me, out, o...|       0.0|\n",
      "|[i, got, you,, pal.]|       0.0|\n",
      "|       [thank, you.]|       0.0|\n",
      "|[your, eyes, are,...|       0.0|\n",
      "|[tears, of, joy.,...|       0.0|\n",
      "|[you, do, somethi...|       2.0|\n",
      "|[we’re, due, at, ...|       2.0|\n",
      "|[no, --, to, the,...|       1.0|\n",
      "|[you’ll, see., , ...|       0.0|\n",
      "|[you’ll, have, to...|       0.0|\n",
      "|[that’s, a, mouth...|       0.0|\n",
      "|[look,, mr., coul...|       2.0|\n",
      "|[well,, great,, i...|       0.0|\n",
      "|[i’m, sure, he, w...|       2.0|\n",
      "|[i...can’t, do, t...|       0.0|\n",
      "|[no,, i, don’t, w...|       0.0|\n",
      "|[yes., , that’s, ...|       0.0|\n",
      "|[what, happened, ...|       1.0|\n",
      "|[uhh,, weren’t, w...|       0.0|\n",
      "|[the, system, is,...|       1.0|\n",
      "|[in, the, coming,...|       1.0|\n",
      "|[you, mean, that?...|       1.0|\n",
      "|   [wait, and, see.]|       0.0|\n",
      "|[i, don’t, want, ...|       0.0|\n",
      "|[we, could, devel...|       0.0|\n",
      "|    [no, it, isn’t.]|       0.0|\n",
      "|[yes., , thanks, ...|       1.0|\n",
      "|   [hello,, jarvis.]|       0.0|\n",
      "|[...i, need, to, ...|       1.0|\n",
      "|[give, me, a, sca...|       1.0|\n",
      "|[it, powers, an, ...|       0.0|\n",
      "|[what, are, you, ...|       1.0|\n",
      "|[upgrade, recomme...|       0.0|\n",
      "|[because, you, ar...|       1.0|\n",
      "|[no., , it’s, ok....|       1.0|\n",
      "|[were, you, alway...|       0.0|\n",
      "|[fine., , could, ...|       0.0|\n",
      "|[ahh, that’s, mor...|       0.0|\n",
      "|    [machine, away.]|       0.0|\n",
      "|[yes., , i, remem...|       0.0|\n",
      "|[i, know, this, i...|       2.0|\n",
      "|        [not, sure.]|       0.0|\n",
      "|[pepper?, , how, ...|       0.0|\n",
      "|[agent, coulson,,...|       1.0|\n",
      "|[how, big, are, y...|       0.0|\n",
      "| [i, don’t, under--]|       0.0|\n",
      "|[--, just, get, d...|       0.0|\n",
      "|             [what?]|       0.0|\n",
      "|[just, show, me, ...|       0.0|\n",
      "|[so, that’s, the,...|       0.0|\n",
      "|[that’s, the, thi...|       1.0|\n",
      "|          [amazing.]|       0.0|\n",
      "|[i’m, going, to, ...|       0.0|\n",
      "|     [is, it, safe?]|       1.0|\n",
      "|[completely., , f...|       1.0|\n",
      "|[reach, in, to, w...|       1.0|\n",
      "|      [the, socket.]|       2.0|\n",
      "|     [what, socket?]|       1.0|\n",
      "|[the, chest, sock...|       2.0|\n",
      "|   [or, else, what?]|       0.0|\n",
      "|[i, can, go, into...|       0.0|\n",
      "|[i, thought, you,...|       0.0|\n",
      "|[i, didn’t, want,...|       0.0|\n",
      "|    [oh, my, god...]|       0.0|\n",
      "|[stay, with, me.,...|       1.0|\n",
      "|[i, don’t, know, ...|       0.0|\n",
      "|[i’m, telling, you.]|       0.0|\n",
      "|          [sorry...]|       0.0|\n",
      "|[listen., , i’m, ...|       1.0|\n",
      "|[won’t, that, mak...|       0.0|\n",
      "|[not, immediately...|       1.0|\n",
      "|   [to, the, right.]|       0.0|\n",
      "|[to, my, right., ...|       0.0|\n",
      "|    [to, the, left.]|       0.0|\n",
      "|            [right.]|       0.0|\n",
      "|             [left.]|       0.0|\n",
      "|   [right., , left.]|       0.0|\n",
      "|[how, deep, does,...|       0.0|\n",
      "|[keep, going., th...|       2.0|\n",
      "|       [ew!!!, pus!]|       0.0|\n",
      "|[it’s, not, pus.,...|       1.0|\n",
      "|[well, it, smells...|       0.0|\n",
      "| [yes., thank, you.]|       0.0|\n",
      "|[can, i, wash, my...|       1.0|\n",
      "|[the, new, unit, ...|       1.0|\n",
      "|[good,, cause, it...|       0.0|\n",
      "|      [it, is, now.]|       0.0|\n",
      "|[i, don’t, suppos...|       0.0|\n",
      "|[can, it, at, lea...|       2.0|\n",
      "|       [i, suppose.]|       0.0|\n",
      "|[throw, that, thi...|       2.0|\n",
      "|[don’t, you, want...|       0.0|\n",
      "|[why?, , it’s, an...|       0.0|\n",
      "|[you, made, it, o...|       2.0|\n",
      "|[pepper., , i, ha...|       1.0|\n",
      "|[you’re, welcome....|       1.0|\n",
      "|            [shoot.]|       0.0|\n",
      "|[i, don’t, do, we...|       1.0|\n",
      "|[i, don’t, have, ...|       0.0|\n",
      "|[will, that, be, ...|       1.0|\n",
      "|[that, will, be, ...|       0.0|\n",
      "|[these, aren’t, f...|       1.0|\n",
      "|[we’ll, start, of...|       0.0|\n",
      "|[great., , i,, uh...|       0.0|\n",
      "|[manned, or, unma...|       0.0|\n",
      "|[why, not, take, ...|       1.0|\n",
      "|[that, i’d, like,...|       1.0|\n",
      "|[who, wants, to, ...|       2.0|\n",
      "|[all, right, --, ...|       1.0|\n",
      "|         [why, not?]|       0.0|\n",
      "|[figured, you’d, ...|       2.0|\n",
      "|[why, does, every...|       1.0|\n",
      "|[you’ve, been, th...|       0.0|\n",
      "|[i’ve, got, it, s...|       0.0|\n",
      "|           [really?]|       0.0|\n",
      "|[i’m, onto, somet...|       0.0|\n",
      "|[lot, of, people,...|       1.0|\n",
      "|[i, mean, what, i...|       2.0|\n",
      "|[no, you, don’t.,...|       2.0|\n",
      "|[maybe, i, do, ne...|       2.0|\n",
      "|[all, right, then...|       1.0|\n",
      "|         [likewise.]|       0.0|\n",
      "|[thought, you, we...|       0.0|\n",
      "|[it’s, a, flight-...|       1.0|\n",
      "|[well,, watch, wh...|       0.0|\n",
      "|[be, right, up., ...|       1.0|\n",
      "|[you, know,, i, h...|       1.0|\n",
      "|[i, know,, i’m, s...|       0.0|\n",
      "|             [what?]|       0.0|\n",
      "|[how, the, hell, ...|       0.0|\n",
      "|[with, the, amoun...|       0.0|\n",
      "|[nothing, to, it....|       0.0|\n",
      "|[we’ll, do, them,...|       0.0|\n",
      "|[that’s, why, you...|       0.0|\n",
      "|[never, interrupt...|       1.0|\n",
      "|[whatever, floats...|       2.0|\n",
      "|[uhh,, jarvis?, ,...|       0.0|\n",
      "|[jarvis,, i, thin...|       0.0|\n",
      "|[you’re, a, downe...|       1.0|\n",
      "|[no,, i, got, it,...|       0.0|\n",
      "|[yeah,, and, it, ...|       0.0|\n",
      "|[re-configure, us...|       1.0|\n",
      "|[wow, me., hm., ,...|       2.0|\n",
      "|[good., , i, shou...|       0.0|\n",
      "|[it’s, time, to, ...|       0.0|\n",
      "|[great., see, ya,...|       0.0|\n",
      "|[oh...was, i, sup...|       0.0|\n",
      "|[yes,, you’re, ri...|       1.0|\n",
      "|[thanks., , it, w...|       0.0|\n",
      "|[i, have, great, ...|       1.0|\n",
      "|[no,, i, always, ...|       0.0|\n",
      "|[would, it, help,...|       0.0|\n",
      "|[you, wouldn’t, l...|       0.0|\n",
      "|[i’m, not, so, su...|       0.0|\n",
      "|[what’s, your, so...|       1.0|\n",
      "|             [uh...]|       0.0|\n",
      "|[119-64-5484, i’m...|       2.0|\n",
      "|[can, i, get, you...|       0.0|\n",
      "|[a, vodka, martin...|       0.0|\n",
      "|             [okay.]|       2.0|\n",
      "|[and,, tony..., ,...|       0.0|\n",
      "|[no., , you’re, n...|       0.0|\n",
      "|     [how’s, panic?]|       0.0|\n",
      "|[hey,, they, just...|       0.0|\n",
      "|[when, were, thes...|       0.0|\n",
      "|[i, didn’t, appro...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 300 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(hash_MCU)\n",
    "predictionFinal_mcu = prediction.select(\n",
    "    \"new_line\", \"prediction\")\n",
    "predictionFinal_mcu.show(n=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0| 4188|\n",
      "|       1.0| 1504|\n",
      "|       2.0|  817|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = predictionFinal_mcu.groupBy('prediction').count()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
