{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data: https://www.kaggle.com/pdunton/marvel-cinematic-universe-dialogue?select=mcu_subset.csv\n",
    "data NRC : https://www.kaggle.com/andradaolteanu/bing-nrc-afinn-lexicons?select=NRC.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover, RegexTokenizer, PCA\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from IPython.display import Image\n",
    "from pyspark.sql import SparkSession\n",
    "import IPython\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting The Infinity Stones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AKA Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![display image](https://media.giphy.com/media/3oxHQjRHcp4w9oi24M/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()\n",
    "#read_csv = spark.read.csv('data/tweets.csv', inferSchema=True, header=True)\n",
    "read_csv = spark.read.csv('data/Reddit_Data_utf8.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|       clean_comment|label|\n",
      "+--------------------+-----+\n",
      "|surprised modis i...|    1|\n",
      "|     naga downs ing |    0|\n",
      "| has been decided...|    0|\n",
      "|how difficult was...|    1|\n",
      "|yes now are going...|    0|\n",
      "|every question yo...|    0|\n",
      "|  the plot thickens |    0|\n",
      "|this compelling y...|    2|\n",
      "|video showing mh3...|    0|\n",
      "|brace yourself th...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data = read_csv.select(\"SentimentText\", col(\"Sentiment\").cast(\"Int\").alias(\"label\"))\n",
    "data = read_csv.select(\"clean_comment\", col(\"category\").cast(\"Int\").alias(\"label\")).dropna().dropDuplicates().replace(-1,2).limit(1000)\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 716 rows.\n",
      "Testing data has 284 rows.\n"
     ]
    }
   ],
   "source": [
    "split = data.randomSplit([0.7, 0.3])\n",
    "trainingData = split[0]\n",
    "testingData = split[1]\n",
    "print (\"Training data has\", split[0].count(), 'rows.')\n",
    "print (\"Testing data has\", split[1].count(), 'rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data (Tokenizing and Stop Word Removing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|       clean_comment|label|              Tokens|         NoStopWords|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "| 10lac suit 100rs...|    0|[10lac, suit, 100...|[10lac, suit, 100...|\n",
      "| 2015 utc 23modi ...|    0|[2015, utc, 23mod...|[2015, utc, 23mod...|\n",
      "| about economics ...|    1|[about, economics...|[economics, gujar...|\n",
      "| adam sniff knowi...|    1|[adam, sniff, kno...|[adam, sniff, kno...|\n",
      "| aerospace engine...|    0|[aerospace, engin...|[aerospace, engin...|\n",
      "| all the signs em...|    1|[all, the, signs,...|[signs, emergency...|\n",
      "| also appreciate ...|    1|[also, appreciate...|[also, appreciate...|\n",
      "| also said easy f...|    1|[also, said, easy...|[also, said, easy...|\n",
      "|     are the legion |    0|  [are, the, legion]|            [legion]|\n",
      "| are you saying t...|    0|[are, you, saying...|[saying, tally, b...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|       clean_comment|label|              Tokens|         NoStopWords|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "| 5ppr for flex sp...|    0|[5ppr, for, flex,...|[5ppr, flex, spot...|\n",
      "| add some context...|    1|[add, some, conte...|[add, context, ec...|\n",
      "| all the people w...|    1|[all, the, people...|[people, try, mak...|\n",
      "| and honest seen ...|    1|[and, honest, see...|[honest, seen, si...|\n",
      "| anything would m...|    1|[anything, would,...|[anything, make, ...|\n",
      "| bjp ministers wh...|    0|[bjp, ministers, ...|[bjp, ministers, ...|\n",
      "| breaking news ma...|    0|[breaking, news, ...|[breaking, news, ...|\n",
      "| constitution nee...|    0|[constitution, ne...|[constitution, ne...|\n",
      "| day will come wh...|    0|[day, will, come,...|[day, come, pay, ...|\n",
      "| didn also said s...|    1|[didn, also, said...|[didn, also, said...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inputCol = \"SentimentText\"\n",
    "inputCol = \"clean_comment\"\n",
    "\n",
    "tokenizer = RegexTokenizer(pattern=r'(?:\\p{Punct}|\\s)+', inputCol=inputCol, outputCol=\"Tokens\")\n",
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"NoStopWords\")\n",
    "\n",
    "token_train = tokenizer.transform(trainingData)\n",
    "nosw_train = swr.transform(token_train)\n",
    "\n",
    "token_test = tokenizer.transform(testingData)\n",
    "nosw_test = swr.transform(token_test)\n",
    "\n",
    "nosw_train.show(truncate=True, n=10)\n",
    "nosw_test.show(truncate=True, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing The Features using HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|              Tokens|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[10lac, suit, 100...|(262144,[90138,18...|\n",
      "|    0|[2015, utc, 23mod...|(262144,[30914,75...|\n",
      "|    1|[about, economics...|(262144,[32958,49...|\n",
      "|    1|[adam, sniff, kno...|(262144,[4757,288...|\n",
      "|    0|[aerospace, engin...|(262144,[216916,2...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+--------------------+\n",
      "|Label|              Tokens|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[5ppr, for, flex,...|(262144,[5972,147...|\n",
      "|    1|[add, some, conte...|(262144,[2306,853...|\n",
      "|    1|[all, the, people...|(262144,[41809,61...|\n",
      "|    1|[and, honest, see...|(262144,[8807,323...|\n",
      "|    1|[anything, would,...|(262144,[16293,34...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "hash_train = hashTF.transform(nosw_train).select(\n",
    "    'label', 'Tokens', 'features')\n",
    "\n",
    "hash_test = hashTF.transform(nosw_test).select(\n",
    "    'Label', 'Tokens', 'features')\n",
    "hash_train.show(n=5)\n",
    "hash_test.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlor = (LogisticRegression()\n",
    "       .setFamily(\"multinomial\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= mlor.fit(hash_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = model.transform(hash_test)\n",
    "#prediction.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+\n",
      "|              Tokens|prediction|Label|\n",
      "+--------------------+----------+-----+\n",
      "|[5ppr, for, flex,...|       1.0|    0|\n",
      "|[add, some, conte...|       1.0|    1|\n",
      "|[all, the, people...|       1.0|    1|\n",
      "|[and, honest, see...|       1.0|    1|\n",
      "|[anything, would,...|       0.0|    1|\n",
      "|[bjp, ministers, ...|       1.0|    0|\n",
      "|[breaking, news, ...|       0.0|    0|\n",
      "|[constitution, ne...|       1.0|    0|\n",
      "|[day, will, come,...|       0.0|    0|\n",
      "|[didn, also, said...|       1.0|    1|\n",
      "|[don, see, how, t...|       0.0|    2|\n",
      "|[don, see, why, n...|       2.0|    1|\n",
      "|[echo, the, senti...|       1.0|    1|\n",
      "|[explained, this,...|       1.0|    2|\n",
      "|[feel, like, part...|       0.0|    0|\n",
      "|[feel, podcasts, ...|       1.0|    1|\n",
      "|[feels, this, cou...|       1.0|    1|\n",
      "|[few, qustions, w...|       1.0|    1|\n",
      "|[game, just, incr...|       1.0|    1|\n",
      "|[gentle, why, was...|       1.0|    0|\n",
      "|   [get, girlfriend]|       0.0|    0|\n",
      "|[good, see, him, ...|       1.0|    1|\n",
      "|[guys, this, for,...|       1.0|    1|\n",
      "|[hai, aam, modi, ...|       0.0|    2|\n",
      "|[have, been, plea...|       1.0|    2|\n",
      "|[have, say, this,...|       1.0|    2|\n",
      "|[here, guess, thi...|       0.0|    0|\n",
      "|[his, daughter, h...|       1.0|    1|\n",
      "|[hope, gets, payt...|       1.0|    1|\n",
      "|        [immigrants]|       0.0|    0|\n",
      "|[india, makes, so...|       1.0|    0|\n",
      "|[indians, should,...|       1.0|    1|\n",
      "|[late, where, fir...|       1.0|    1|\n",
      "|[lost, count, the...|       0.0|    2|\n",
      "|[made, stop, watc...|       1.0|    1|\n",
      "|[maybe, question,...|       1.0|    1|\n",
      "|[might, become, t...|       1.0|    1|\n",
      "|[missed, then, do...|       0.0|    0|\n",
      "|[much, drama, thi...|       1.0|    1|\n",
      "|[never, considere...|       1.0|    1|\n",
      "|[not, new, low, t...|       1.0|    2|\n",
      "|[not, over, bjp, ...|       1.0|    1|\n",
      "|[notice, this, th...|       1.0|    0|\n",
      "|[orwell, called, ...|       2.0|    1|\n",
      "|[overly, attached...|       0.0|    0|\n",
      "|[parents, believe...|       1.0|    2|\n",
      "|[physics, was, to...|       0.0|    0|\n",
      "|[prescient, trump...|       2.0|    0|\n",
      "|[prime, minister,...|       2.0|    1|\n",
      "|[rather, enjoy, t...|       0.0|    1|\n",
      "|[remember, readin...|       1.0|    1|\n",
      "|[remember, readin...|       2.0|    0|\n",
      "|[reply, will, pro...|       1.0|    1|\n",
      "|[saal, kejriwal, ...|       0.0|    0|\n",
      "|             [saida]|       0.0|    0|\n",
      "|[seunghwan, from,...|       0.0|    0|\n",
      "|[something, final...|       0.0|    0|\n",
      "|[swamp, status, d...|       1.0|    0|\n",
      "|[take, jab, perso...|       1.0|    1|\n",
      "|[tava, transborda...|       0.0|    0|\n",
      "|[tejasvi, haripra...|       0.0|    2|\n",
      "|[thanks, very, sm...|       1.0|    1|\n",
      "|[the, indian, exp...|       1.0|    2|\n",
      "|[the, same, every...|       0.0|    0|\n",
      "|[there, any, way,...|       1.0|    2|\n",
      "|[these, are, some...|       2.0|    2|\n",
      "|[think, healthier...|       1.0|    1|\n",
      "|[think, there, sh...|       0.0|    2|\n",
      "|[this, only, make...|       0.0|    1|\n",
      "|[this, point, mod...|       0.0|    1|\n",
      "|[this, surrogate,...|       0.0|    0|\n",
      "|[this, throws, op...|       0.0|    2|\n",
      "|[thought, jio, wo...|       1.0|    1|\n",
      "|[true, then, bad,...|       1.0|    2|\n",
      "|[understand, gett...|       1.0|    2|\n",
      "|[was, also, when,...|       1.0|    1|\n",
      "|[was, museum, tod...|       0.0|    2|\n",
      "|[was, surprised, ...|       1.0|    1|\n",
      "|[was, watching, t...|       2.0|    2|\n",
      "|[welcome, begin, ...|       1.0|    1|\n",
      "|[which, date, and...|       0.0|    0|\n",
      "|[will, start, pre...|       0.0|    2|\n",
      "|[will, travelling...|       0.0|    1|\n",
      "|[wish, this, was,...|       1.0|    1|\n",
      "|[with, waiting, u...|       1.0|    1|\n",
      "|[wouldn, surprise...|       1.0|    1|\n",
      "|[you, did, not, l...|       1.0|    1|\n",
      "|[you, really, wan...|       1.0|    1|\n",
      "|[you, remember, t...|       0.0|    0|\n",
      "|[you, think, isla...|       1.0|    0|\n",
      "|[you, think, the,...|       1.0|    1|\n",
      "|[aaaand, the, str...|       0.0|    0|\n",
      "|[abhinandan, just...|       1.0|    1|\n",
      "|  [alexa, this, sad]|       0.0|    2|\n",
      "|[also, hates, any...|       0.0|    0|\n",
      "|[and, other, thre...|       1.0|    2|\n",
      "|[and, this, assho...|       2.0|    0|\n",
      "|[any, ideas, for,...|       1.0|    0|\n",
      "|[apologise, that,...|       0.0|    0|\n",
      "|[arnab, has, inte...|       0.0|    1|\n",
      "+--------------------+----------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionFinal = prediction.select(\n",
    "    \"Tokens\", \"prediction\", \"Label\")\n",
    "predictionFinal.show(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.573943661971831\n"
     ]
    }
   ],
   "source": [
    "match = predictionFinal.filter(predictionFinal['prediction'] == predictionFinal['label']).count()\n",
    "total = predictionFinal.count()\n",
    "print(\"Accuracy:\", match/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = predictionFinal.drop('Tokens')\n",
    "\n",
    "# predictionAndLabels = temp.rdd.map(lambda lp: (float(prediciton), lp.label))\n",
    "# predictionAndLabels = temp.rdd.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "#create evaluators\n",
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"Label\", predictionCol=\"prediction\")\n",
    "\n",
    "# get metrics\n",
    "f1 = evaluatorMulti.evaluate(temp, {evaluatorMulti.metricName: \"f1\"})\n",
    "weightedPrecision = evaluatorMulti.evaluate(temp, {evaluatorMulti.metricName: \"weightedPrecision\"})\n",
    "weightedRecall = evaluatorMulti.evaluate(temp, {evaluatorMulti.metricName: \"weightedRecall\"})\n",
    "\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % weightedPrecision)\n",
    "print(\"Recall = %s\" % weightedRecall)\n",
    "print(\"F1 Score = %s\" % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "# # Overall statistics\n",
    "# precision = metrics.precision(1.0)\n",
    "# recall = metrics.recall(1.0)\n",
    "# f1Score = metrics.fMeasure(1.0)\n",
    "# print(\"Summary Stats\")\n",
    "# print(\"Precision = %s\" % precision)\n",
    "# print(\"Recall = %s\" % recall)\n",
    "# print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avengers Assemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![display image](https://media.giphy.com/media/j2pWZpr5RlpCodOB0d/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcu_csv = spark.read.csv('data/mcu_subset.csv', inferSchema=True, header=True)\n",
    "print(\"Lines of Dialogue:\", mcu_csv.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mcu_csv.select(\"character\",\"line\")\n",
    "data.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(inputCol=\"line\", outputCol=\"new_line\")\n",
    "swr_MCU = StopWordsRemover(inputCol=t.getOutputCol(), \n",
    "                       outputCol=\"new\")\n",
    "token_MCU = t.transform(data)\n",
    "nosw_MCU = swr_MCU.transform(token_MCU)\n",
    "\n",
    "nosw_MCU.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashTF = HashingTF(inputCol=swr_MCU.getOutputCol(), outputCol=\"features\")\n",
    "hash_MCU = hashTF.transform(nosw_MCU).select('new_line', 'features')\n",
    "hash_MCU.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(hash_MCU)\n",
    "predictionFinal_mcu = prediction.select(\n",
    "    \"new_line\", \"prediction\")\n",
    "predictionFinal_mcu.show(n=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = predictionFinal_mcu.groupBy('prediction').count()\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
